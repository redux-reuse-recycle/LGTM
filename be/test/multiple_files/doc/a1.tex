\documentclass{article}

\usepackage[procnames]{listings}
\usepackage{hyperref}

\usepackage[]{algorithm2e}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}

% Math
\def\norm#1{\|#1\|}
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

% Listings
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class},
	extendedchars=true,
    	literate={Î»}{{$\lambda$}}1,}

\begin{document}

\title{CPSC 340 Assignment 1}

\date{}
\maketitle

\vspace{-4em}

\textbf{Name:} Zhi Ting (Jessica) Yang

\textbf{Student \#:} 33586158

\textbf{CS ID:} c5m0b
 
\section{Linear Algebra Review}

\subsection{Basic Operations}
\rubric{reasoning:7}

Use the definitions below,
\[
\alpha = 2,\quad
x = \left[\begin{array}{c}
0\\
1\\
2\\
\end{array}\right], \quad
y = \left[\begin{array}{c}
3\\
4\\
5\\
\end{array}\right],\quad
z = \left[\begin{array}{c}
1\\
2\\
-1\\
\end{array}\right],\quad
A = \left[\begin{array}{ccc}
3 & 2 & 2\\
1 & 3 & 1\\
1 & 1 & 3
\end{array}\right],
\]
and use $x_i$ to denote element $i$ of vector $x$.
\blu{Evaluate the following expressions} (you do not need to show your work).
\enum{
\item $\sum_{i=1}^n x_iy_i$ (inner product).
\item[] \gre{= 14}
\item $\sum_{i=1}^n x_iz_i$ (inner product between orthogonal vectors).
\item[] \gre{= 0}
\item $\alpha(x+z)$ (vector addition and scalar multiplication)
\item[] \gre{\[
= \left[ {\begin{array}{c}
2 \\
6 \\
2 \\
\end{array} } \right]
\]}
\item $x^Tz + \norm{x}$ (inner product in matrix notation and Euclidean norm of $x$).
\item[] \gre{= $\sqrt{5}$}
\item $Ax$ (matrix-vector multiplication).
\item[] \gre{\[
= \left[ {\begin{array}{c}
6 \\
5 \\
7 \\
\end{array} } \right]
\]}
\item $x^TAx$ (quadratic form).
\item[] \gre{= 19}
\item $A^TA$ (matrix tranpose and matrix multiplication).
\item[] \gre{\[
= \left[\begin{array}{ccc}
11 & 10 & 10\\
10 & 14 & 10\\
10 & 10 & 14
\end{array}\right]
\]}
}
\subsection{Matrix Algebra Rules}
\rubric{reasoning:10}

Assume that $\{x,y,z\}$ are $n \times 1$ column vectors, $\{A,B,C\}$ are $n \times n$ real-valued matrices, \red{$0$ is the zero matrix of appropriate size}, and $I$ is the identity matrix of appropriate size. \blu{State whether each of the below is true in general} (you do not need to show your work).

\begin{enumerate}
\item $x^Ty = \sum_{i=1}^n x_iy_i$.
\item[] \gre{True}
\item $x^Tx = \norm{x}^2$.
\item[] \gre{True}
\item $x^Tx = xx^T$.
\item[] \gre{False}
\item $(x-y)^T(x-y) = \norm{x}^2 - 2x^Ty + \norm{y}^2$.
\item[] \gre{True}
\item $AB=BA$.
\item[] \gre{False}
\item $A^T(B + IC) = A^TB + A^TC$.
\item[] \gre{True}
\item $(A + BC)^T = A^T + B^TC^T$.
\item[] \gre{False}
\item $x^TAy = y^TA^Tx$.
\item[] \gre{True}
\item $A^TA = AA^T$ if $A$ is a symmetric matrix.
\item[] \gre{True}
\item $A^TA = 0$ if the columns of $A$ are orthonormal.
\item[] \gre{False}
\end{enumerate}

\section{Probability Review}

\subsection{Rules of probability}
\rubric{reasoning:6}

\blu{Answer the following questions.} You do not need to show your work.

\begin{enumerate}
\item You are offered the opportunity to play the following game: your opponent rolls 2 regular 6-sided dice. If the difference between the two rolls is at least 3, you win \$15. Otherwise, you get nothing. What is a fair price for a ticket to play this game once? In other words, what is the expected value of playing the game?
\item[] \gre{\$5}
\item Consider two events $A$ and $B$ such that $\Pr(A, B)=0$ (they are mutually exclusive). If $\Pr(A) = 0.4$ and $\Pr(A \cup B) = 0.95$, what is $\Pr(B)$? Note: $p(A, B)$ means
``probability of $A$ and $B$'' while $p(A \cup B)$ means ``probability of $A$ or $B$''. It may be helpful to draw a Venn diagram.
\item[] \gre{0.55}
\item Instead of assuming that $A$ and $B$ are mutually exclusive ($\Pr(A,B) = 0)$, what is the answer to the previous question if we assume that $A$ and $B$ are independent?
\item[] \gre{$\frac{0.55}{0.6}$}

\end{enumerate}

\subsection{Bayes Rule and Conditional Probability}
\rubric{reasoning:10}

\blu{Answer the following questions.} You do not need to show your work.

Suppose a drug test produces a positive result with probability $0.97$ for drug users, $P(T=1 \mid D=1)=0.97$. It also produces a negative result with probability $0.99$ for non-drug users, $P(T=0 \mid D=0)=0.99$. The probability that a random person uses the drug is $0.0001$, so $P(D=1)=0.0001$.

\begin{enumerate}
\item What is the probability that a random person would test positive, $P(T=1)$?
\item[] \gre{0.010096}
\item In the above, do most of these positive tests come from true positives or from false positives?
\item[] \gre{Most of the positive tests come from false positives.}
\item What is the probability that a random person who tests positive is a user, $P(D=1 \mid T=1)$?
\item[] \gre{0.009607765}
\item Suppose you have given this test to a random person and it came back positive, are they likely to be a drug user?
\item[] \gre{No, as most of the positive tests come from false positives. The chance of someone actually being a drug user when testing positive is only about 1\%.}
\item What is one factor you could change to make this a more useful test?
\item[] \gre{Decreasing the probability of a positive result for non-drug users (decreasing $P(T=1 \mid D=0)$) would help increase the accuracy of the test.}
\end{enumerate}

\section{Calculus Review}

\subsection{One-variable derivatives}
\label{sub.one.var}
\rubric{reasoning:8}

\blu{Answer the following questions.} You do not need to show your work.

\begin{enumerate}
\item Find the derivative of the function $f(x) = 3x^2 -2x + 5$.
\item[] \gre{$f'(x) = 6x - 2$}
\item Find the derivative of the function $f(x) = x(1-x)$.
\item[] \gre{$f'(x) = 1 - 2x$}
\item Let $p(x) = \frac{1}{1+\exp(-x)}$ for $x \in \R$. Compute the derivative of the function $f(x) = x-\log(p(x))$ and simplify it by using the function $p(x)$.
\item[] \gre{$f'(x) = p(x)$}
\end{enumerate}
Remember that in this course we will $\log(x)$ to mean the ``natural'' logarithm of $x$, so that $\log(\exp(1)) = 1$. Also, obseve that $p(x) = 1-p(-x)$ for the final part.

\subsection{Multi-variable derivatives}
\label{sub.multi.var}
\rubric{reasoning:5}

\blu{Compute the gradient vector $\nabla f(x)$ of each of the following functions.} You do not need to show your work.
\begin{enumerate}
\item $f(x) = x_1^2 + \exp(x_1 + 2x_2)$ where $x \in \R^2$.
\item[] \gre{$\nabla f(x) = <2x_1 + \exp(x_1 + 2x_2), 2exp(x_1 + 2x_2)>$} 
\item $f(x) = \log\left(\sum_{i=1}^3\exp(x_i)\right)$ where $x \in \R^3$ (simplify the gradient by defining $Z = \sum_{i=1}^3\exp(x_i)$).
\item[] \gre{$\nabla f(x) = \frac{1}{Z}<\exp(x_1), \exp(x_2), \exp(x_3))>$} 
\item $f(x) = a^Tx + b$ where $x \in \R^3$ and $a \in \R^3$ and $b \in \R$.
\item[] \gre{$\nabla f(x) = <a_1, a_2, a_3> = a$} 
\item $f(x) = \half x^\top A x$ where $A=\left[ \begin{array}{cc}
2 & -1 \\
 -1 & 2 \end{array} \right]$ and $x \in \mathbb{R}^2$.
\item[] \gre{$\nabla f(x) = <2x_1 - x_2, -x_1 + 2x_2> = Ax$}
\item $f(x) = \frac{1}{2}\norm{x}^2$ where $x \in \R^d$.
\item[] \gre{$\nabla f(x) = <x_1, x_2,..., x_n> = x$} 
\end{enumerate}

\subsection{Optimization}
\blu{Find the following quantities.} You do not need to show your work. 
You can/should use your results from parts \ref{sub.one.var} and \ref{sub.multi.var} as part of the process.

\begin{enumerate}
\item $\min \, 3x^2-2x+5$, or, in words, the minimum value of the function $f(x) = 3x^2 -2x + 5$ for $x \in \R$.
\item[] \gre{$\frac{14}{3}$}
\item $\max \, x(1-x)$ for $x\in [0,1]$.
\item[] \gre{$\frac{1}{4}$}
\item $\min \, x(1-x)$ for $x\in [0,1]$.
\item[] \gre{0}
\item $\arg \max \, x(1-x)$ for $x\in[0,1]$.
\item[] \gre{$x = \frac{1}{2}$}
\item $\min \, x_1^2 + \exp(x_2)$ where $x \in [0,1]^2$, or in other words $x_1\in [0,1]$ and $x_2\in [0,1]$.
\item[] \gre{1}
\item $\arg \min \, x_1^2 + \exp(x_2)$ where $x \in [0,1]^2$.
\item[] \gre{$x_1 = 0, x_2 = 0$}
\end{enumerate}

\subsection{Derivatives of code}

\rubric{code:4}
 
\begin{lstlisting}
def foo_grad(x):
    return 4*x**3

def bar_grad(x):
    grad = np.ones(len(x))
    for i in range(len(x)):
        for j in range(len(x)):
            if i != j:
                grad[i] = grad[i] * x[j]
    return grad
\end{lstlisting}

\gre{See \hyperlink{3.4}{\texttt{grads.py}} for full code and \hyperlink{main.py}{\texttt{main.py}} for tests}

\section{Algorithms and Data Structures Review}

\subsection{Trees}
\rubric{reasoning:2}

\blu{Answer the following questions.} You do not need to show your work.

\begin{enumerate}
\item What is the minimum depth of a binary tree with 64 leaf nodes?
\item[] \gre{6}
\item What is the minimum depth of binary tree with 64 nodes (includes leaves and all other nodes)?
\item[] \gre{6}
\end{enumerate}
Note: we'll use the standard convention that the leaves are not included in the depth, so a tree with depth $1$ has 3 nodes with 2 leaves.

\subsection{Common Runtimes}
\rubric{reasoning:4}

\blu{Answer the following questions using big-$O$ notation} You do not need to show your work.
\begin{enumerate}
\item What is the cost of finding the largest number in an unsorted list of $n$ numbers?
\item[] \gre{$O(n)$}
\item What is the cost of finding the smallest element greater than 0 in a \emph{sorted} list with $n$ numbers.
\item[] \gre{$O(log(n))$}
\item What is the cost of finding the value associated with a key in a hash table with $n$ numbers? \\(Assume the values and keys are both scalars.)
\item[] \gre{$O(1)$}
\item What is the cost of computing the inner product $a^Tx$, where $a$ is $d \times 1$ and $x$ is $d \times 1$?
\item[] \gre{$O(d)$}
\item What is the cost of computing the quadratic form $x^TAx$ when $A$ is $d \times d$ and $x$ is $d \times 1$.
\item[] \gre{$O(d^2)$}
\end{enumerate}

\subsection{Running times of code}
\rubric{reasoning:4}

Your repository contains a file named \texttt{bigO.py}, which defines several functions
that take an integer argument $N$. For each function, \blu{state the running time as a function of $N$, using big-O notation}.
Please include your answers in your report. Do not write your answers inside \texttt{bigO.py}.

\gre{\texttt{func1} is $O(N)$}

\gre{\texttt{func2} is $O(N)$}

\gre{\texttt{func3} is $O(1)$}

\gre{\texttt{func4} is $O(N^2)$}

\section{Data Exploration}

\subsection{Summary Statistics}
\rubric{reasoning:2}

\blu{Report the following statistics}:

\gre{See code used to calculate the below at \hyperlink{main.py}{\texttt{main.py}}}
\enum{
\item The minimum, maximum, mean, median, and mode of all values across the dataset.
\item[] \gre{Minimum: 0.35200000000000004}
\item[] \gre{Maximum: 4.862}
\item[] \gre{Mean: 1.3246250000000002}
\item[] \gre{Median: 1.1589999999999998}
\item[] \gre{Mode: 0.77}
\item The $5\%$, $25\%$, $50\%$, $75\%$, and $95\%$ quantiles of all values across the dataset.
\item[] \gre{5\% quantile: 0.46495000000000003}
\item[] \gre{25\% quantile: 0.718}
\item[] \gre{50\% quantile: 1.1589999999999998}
\item[] \gre{75\% quantile: 1.81325}
\item[] \gre{95\% quantile: 2.624049999999999}
\item The names of the regions with the highest and lowest means, and the highest and lowest variances.
\item[] \gre{The region with the highest mean: WtdILI}
\item[] \gre{The region with the lowest mean: Pac}
\item[] \gre{The region with the highest var: Mtn}
\item[] \gre{The region with the lowest var: Pac}
}
In light of the above, \blu{is the mode a reliable estimate of the most ``common" value? Describe another way we could give a meaningful ``mode" measurement for this (continuous) data.} Note: the function \texttt{utils.mode()} will compute the mode value of an array for you.

\gre{The mode is not always a reliable estimate of the most "common" value for continuous data, as it does not take account how "close together" values are to each other, only whether they are the exact same value. The mode was 0.77 as that particular exact value just happened to appear in the data the most times.}

\gre{A way we could compute a more meaningful "mode" measurement for this data is to bin the data by rounding to 1 decimal place, and then taking the mode of the new discretized data.}

\subsection{Data Visualization}
\rubric{reasoning:3}

Consider the figure below.

\fig{1}{../figs/visualize-unlabeled}

The figure contains the following plots, in a shuffled order:
\enum{
\item A single histogram showing the distribution of \emph{each} column in $X$.
\item A histogram showing the distribution of each the values in the matrix $X$.
\item A boxplot grouping data by weeks, showing the distribution across regions for each week.
\item A plot showing the illness percentages over time.
\item A scatterplot between the two regions with highest correlation.
\item A scatterplot between the two regions with lowest correlation.
}

\blu{Match the plots (labeled A-F) with the descriptions above (labeled 1-6), with an extremely brief (a few words is fine) explanation for each decision.}

\gre{Plot A: Description 4. It is the only regular plot, so it has to map to Description 4. It shows the illness percentages by week, so over time.}

\gre{Plot B: Description 3. It is the only boxplot, so it has to map to Description 3. By grouping data by weeks against illness percentage, the distribution over 
regions for each week is shown via each box.}

\gre{Plot C: Description 2. It is a histogram, so it has to map to either Description 1 or 2. It is Description 2 as it does not show the distribution of each region, so it is not Description 1.}

\gre{Plot D: Description 1. See above, this is a histogram that shows the distribution of each region, and each region is a column in $X$.}

\gre{Plot E: Description 6. It is a scatterplot, so it has to map to either Description 5 or 6. The correlation is lower than Plot F, so it maps to Description 6.}

\gre{Plot F: Description 5. See above, here, the correlation is high, so it maps to Description 5.}

\section{Decision Trees}

\subsection{Splitting rule}
\rubric{reasoning:1}

Is there a particular type of features for which it makes sense to use an equality-based splitting rule rather than the threshold-based splits we discussed in class?

\gre{Yes, categorical features, as threshold-based splits wouldn't be possible.}

\subsection{Decision Stump Implementation}
\rubric{code:3}

The file \texttt{decision\string_stump.py} contains the class \texttt{DecisionStumpEquality} which 
finds the best decision stump using the equality rule and then makes predictions using that
rule. Instead of discretizing the data and using a rule based on testing an equality for 
a single feature, we want to check whether a feature is above or below a threshold and 
split the data accordingly (this is a more sane approach, which we discussed in class). 
\blu{Create a \texttt{DecisionStumpErrorRate} class to do this, and report the updated error you 
obtain by using inequalities instead of discretizing and testing equality. Also submit the generated figure of the classification boundary.}

Hint: you may want to start by copy/pasting the contents \texttt{DecisionStumpEquality} and then make modifications from there. 

\begin{lstlisting}
class DecisionStumpErrorRate:

    def __init__(self):
        pass

    # Copied and then modified from DecisionStumpEquality.fit
    def fit(self, X, y):
        # X is N by D, N rows and D columns
        N, D = X.shape

        # Get an array with the number of 0's, number of 1's, etc.
        # count = occurrence of each unique label
        count = np.bincount(y)
        
        # Get the index of the largest value in count.
        # Thus, y_mode is the mode (most popular value) of y
        y_mode = np.argmax(count)

        # Initialize prediction to y_mode
        self.splitSat = y_mode
        self.splitNot = None
        # Feature we split on
        self.splitVariable = None
        # Threshold we split with
        self.splitValue = None

        # If all the labels are the same, no need to split further
        # Always predict y_mode
        if np.unique(y).size <= 1:
            return

        # Initialize minError as number of times we predict wrong if we always predict y_mode
        # This is computing the error if using "baseline" rule, number of times y_i does not equal most common value
        minError = np.sum(y != y_mode)

        # Loop over features looking for the best split
        # No need to round as we are doing a threshold-based split
        # For feature d of D features
        for d in range(D):
            # For example n of N examples
            for n in range(N):
                # Choose value to equate to
                # This is a threshold
                value = X[n, d]

                # Find most likely class for each split
                # Change from equality-based split to threshold-based split
                # Set y_sat to most common label of examples satisfying rule
                y_sat = utils.mode(y[X[:,d] > value])
                # Set y_not to most common label of examples not satisfying rule
                y_not = utils.mode(y[X[:,d] <= value])

                # Make predictions
                y_pred = y_sat * np.ones(N)
                y_pred[X[:, d] <= value] = y_not

                # Compute error
                errors = np.sum(y_pred != y)

                # Compare to minimum error so far
                if errors < minError:
                    # This is the lowest error, store this value
                    minError = errors
                    self.splitVariable = d
                    self.splitValue = value
                    self.splitSat = y_sat
                    self.splitNot = y_not
\end{lstlisting}

\gre{The updated error obtained by using inequalities is 0.253.}

\gre{Generated figure:}
\centerfig{0.7}{../figs/q6_2_decisionBoundary}

\subsection{Decision Stump Info Gain Implementation}
\rubric{code:3}

In \texttt{decision\string_stump.py}, \blu{create a \texttt{DecisionStumpInfoGain} class that 
fits using the information gain criterion discussed in lecture. Report the updated error you obtain, and submit the classification boundary figure.}

Notice how the error rate changed. Are you surprised? If so, hang on until the end of this question!

Note: even though this data set only has 2 classes (red and blue), your implementation should work 
for any number of classes, just like \texttt{DecisionStumpEquality} and \texttt{DecisionStumpErrorRate}.

Hint: take a look at the documentation for \texttt{np.bincount}, at \\
\url{https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html}. 
The \texttt{minlength} argument comes in handy here to deal with a tricky corner case:
when you consider a split, you might not have any examples of a certain class, like class 1,
going to one side of the split. Thus, when you call \texttt{np.bincount}, you'll get
a shorter array by default, which is not what you want. Setting \texttt{minlength} to the 
number of classes solves this problem.

\begin{lstlisting}
# This is not required, but one way to simplify the code is
# to have this class inherit from DecisionStumpErrorRate.
# Which methods (init, fit, predict) do you need to overwrite?

# Only need to overwrite fit as init and predict are the same as DecisionStumpErrorRate
class DecisionStumpInfoGain(DecisionStumpErrorRate):
    
    # Copied and then modified from DecisionStumpErrorRate.fit
    def fit(self, X, y):
        # X is N by D, N rows and D columns
        N, D = X.shape

        # Get an array with the number of 0's, number of 1's, etc.
        # count = occurrence of each unique label
        count = np.bincount(y)
        
        # Get the index of the largest value in count.
        # Thus, y_mode is the mode (most popular value) of y
        y_mode = np.argmax(count)

        # Initialize prediction to y_mode
        self.splitSat = y_mode
        self.splitNot = None
        # Feature we split on
        self.splitVariable = None
        # Threshold we split with
        self.splitValue = None
    
        # Initialize info gain to 0, this is the baseline rule ("do nothing")
        maxInfoGain = 0
        # Get an array with the probabilities of the occurrence of each unique label
        p = count/np.sum(count)
        # Find entropy of labels before split
        preSplitEntropy = entropy(p)

        # If all the labels are the same, no need to split further
        # Always predict y_mode
        if np.unique(y).size <= 1:
            return

        # Loop over features looking for the best split to maximize info gain
        # For feature d of D features
        for d in range(D):
            # For example n of N examples
            for n in range(N):
                # Choose value to equate to
                # This is a threshold
                value = X[n, d]

                examples_satisfying_rule = X[:,d] > value
                # Calculate number of examples satisfying rule / number of examples to get probability of "yes" side
                y_prob = np.sum(examples_satisfying_rule) / N
                # Calculate probability of "no" side
                n_prob = 1 - y_prob

                # Count number of labels satisfying rule
                y_labels = y[examples_satisfying_rule]
                y_count = np.bincount(y_labels, minlength = len(count))
                # Calculate number of labels not satisfying rule
                n_count = count - y_count

                # Calculate entropy of labels for examples satisfying rule
                y_p = y_count / np.sum(y_count)
                y_entropy = entropy(y_p)

                # Calculate entropy of labels for examples not satisfying rule
                n_p = n_count / np.sum(n_count)
                n_entropy = entropy(n_p)

                # Calculate info gain from split
                splitInfoGain = preSplitEntropy - (y_prob * y_entropy) - (n_prob * n_entropy)

                # Store split if info gain is greater than best found so far
                if splitInfoGain > maxInfoGain:
                    self.splitSat = np.argmax(y_count)
                    self.splitNot = np.argmax(n_count)
                    self.splitVariable = d
                    self.splitValue = value
                    # Update new best info gain found so far
                    maxInfoGain = splitInfoGain
\end{lstlisting}

\gre{The updated error obtained by using info gain is 0.325.}

\gre{Generated figure:}
\centerfig{0.7}{../figs/q6_3_decisionBoundary}

\subsection{Constructing Decision Trees}
\rubric{code:2}

Once your \texttt{DecisionStumpInfoGain} class is finished, running \texttt{python main.py -q 6.4} will fit
a decision tree of depth~2 to the same dataset (which results in a lower training error).
Look at how the decision tree is stored and how the (recursive) \texttt{predict} function works.
\blu{Using the splits from the fitted depth-2 decision tree, write a hard-coded version of the \texttt{predict}
function that classifies one example using simple if/else statements
(see the Decision Trees lecture).} Save your code in a new file called
\texttt{simple\string_decision.py} (in the \texttt{code} directory) and make sure you include the text of this file in your \LaTeX \, report.

Note: this code should implement the specific, fixed decision tree
which was learned after calling \texttt{fit} on this particular data set. It does not need to be a learnable model.
You should just hard-code the split values directly into the code. Only the \texttt{predict} function is needed.

Hint: if you plot the decision boundary you can do a visual sanity check to see if your code is consistent with the plot.

\lstinputlisting{../code/simple_decision.py}

\subsection{Decision Tree Training Error}
\rubric{reasoning:2}

Running \texttt{python main.py -q 6.5} fits decision trees of different depths using the following different implementations: 
\enum{
\item A decision tree using \texttt{DecisionStump}
\item A decision tree using \texttt{DecisionStumpInfoGain}
\item The \texttt{DecisionTreeClassifier} from the popular Python ML library \emph{scikit-learn}
}

Run the code and look at the figure.
\blu{Describe what you observe. Can you explain the results?} Why is approach (1) so disappointing? Also, \blu{submit a classification boundary plot of the model with the lowest training error}.

Note: we set the \verb|random_state| because sklearn's \texttt{DecisionTreeClassifier} is non-deterministic. This is probably
because it breaks ties randomly.

Note: the code also prints out the amount of time spent. You'll notice that sklearn's implementation is substantially faster. This is because
our implementation is based on the $O(n^2d)$ decision stump learning algorithm and sklearn's implementation presumably uses the faster $O(nd\log n)$
decision stump learning algorithm that we discussed in lecture.

\gre{\texttt{DecisionTreeErrorRate}'s lowest classification error is about 0.10 which occurs at the 4th split, and further splits do not decrease the classification error. However, both \texttt{DecisionTreeInfoGain} and \texttt{DecisionTreeClassifier} from \emph{scikit-learn} are able to obtain a classification error of 0 given a deep enough tree. \texttt{DecisionTreeErrorRate} cannot obtain a lower classification score after the 4th split as for every leaf, there is no rule that exists that would be better than just predicting the mode.}

\gre{Classification boundary plot of the model with the lowest training error, using \texttt{DecisionTreeClassifier} with depth 10:}

\centerfig{0.7}{../figs/q6_5_lowestTrainingError}

\subsection{Comparing implementations}
\rubric{reasoning:2}

In the previous section you compared different implementations of a machine learning algorithm. Let's say that two
approaches produce the exact same curve of classification error rate vs. tree depth. Does this conclusively demonstrate
that the two implementations are the same? If so, why? If not, what other experiment might you perform to build confidence
that the implementations are probably equivalent?

\gre{No, for example, the implementation of \texttt{DecisionTreeInfoGain} is not the same as sklearn's \texttt{DecisionTreeClassifier}.}

\gre{The classification boundary plots of the \texttt{DecisionTreeInfoGain} vs. \texttt{DecisionTreeClassifier} at a depth of 5 is not the same:}
\centerfig{0.7}{../figs/q6_6_infoGain}
\gre{vs.}
\centerfig{0.7}{../figs/q6_6_classifier}

\gre{Though they both have the same error rate at depth 5, they classify examples differently.}

\gre{An experiment I might perform to build confidence that the implementations are probably equivalent is to see if they classify examples the same. If they both make the same predictions, the implementations are probably the same.}

\subsection{Cost of Fitting Decision Trees}
\rubric{reasoning:3}

In class, we discussed how in general the decision stump minimizing the classification error can be found in $O(nd\log n)$ time.
Using the greedy recursive splitting procedure, \blu{what is the total cost of fitting a decision tree of depth $m$ in terms of $n$, $d$, and $m$?}

Hint: even thought there could be $(2^m-1)$ decision stumps, keep in mind not every stump will need to go through every example. Note also that we stop growing the decision tree if a node has no examples, so we may not even need to do anything for many of the $(2^m-1)$ decision stumps.

\gre{The tree is of depth $m$. There are $n$ examples at the root, and at each iteration, we split the $n$ examples across each stump we generate, so there are $n$ examples at each depth. The cost to build each level of the decision tree is $O(nd\log n)$, the same cost for fitting the root decision stump, as both operations have to look at the same number of examples. The cost for fitting the whole tree of depth $m$ would be $O(mnd\log n)$.}

\section{Appendix}

\subsection[]{Full Code for 3.4}

\hypertarget{3.4}{\texttt{grads.py}}
\lstinputlisting{../code/grads.py}

\subsection[]{Full main.py Code}

\hypertarget{main.py}{\texttt{main.py}}
\lstinputlisting{../code/main.py}

\end{document}
